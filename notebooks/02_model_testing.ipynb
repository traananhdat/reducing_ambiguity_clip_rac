# In a Jupyter Notebook cell

# 02_model_testing.ipynb

# # 1. Setup and Imports
# Import necessary libraries and your custom modules.

import os
import sys
import yaml
import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image

# Add src to path to import custom modules
# Adjust the path as necessary if your notebook is not in the project root
module_path = os.path.abspath(os.path.join('..')) # Assuming notebooks are in a 'notebooks' folder
if module_path not in sys.path:
    sys.path.append(module_path)

# Custom modules
from src.utils.general_utils import load_yaml_config, set_seed, get_device
from src.models.clip_backbone import load_clip_model
from src.models.utils import generate_text_features_for_prompts # You'll need to create this
from src.models.rac_model import RACModel
from src.models.adapters import Adapter, MLP, AlphaGenerator # For testing individual adapters
from src.models.mfa_module import MFAModule
from src.models.ira_module import IRAModule
from src.models.alf_module import ALFModule
from src.models.zs_clip_module import ZeroShotCLIPModule
from src.data_utils.augmentations import get_clip_preprocess # Or your preferred test transform

# Set plot styles
plt.style.use('seaborn-v0_8-whitegrid')

print("Imports completed.")

# # 2. Configuration
# Load a specific configuration for testing. This might be a simplified version or one of your experiment configs.

# Using a relative path from the 'notebooks' directory to 'src/configs'
BASE_CONFIG_PATH = '../src/configs/base_config.yaml'
# Choose a specific model config for testing, e.g., ResNet50
EXPERIMENT_CONFIG_PATH = '../src/configs/rac_resnet50_config.yaml'
# EXPERIMENT_CONFIG_PATH = '../src/configs/rac_vit_b_16_config.yaml' # Or ViT

if not os.path.exists(BASE_CONFIG_PATH) or not os.path.exists(EXPERIMENT_CONFIG_PATH):
    print(f"ERROR: Config files not found. \nBase: {BASE_CONFIG_PATH}\nExperiment: {EXPERIMENT_CONFIG_PATH}")
    print("Please ensure the paths are correct relative to the notebook's location.")
    # sys.exit() # Or handle gracefully

config = {}
try:
    with open(BASE_CONFIG_PATH, 'r') as f:
        base_cfg = yaml.safe_load(f)
    with open(EXPERIMENT_CONFIG_PATH, 'r') as f:
        exp_cfg = yaml.safe_load(f)
    config = {**base_cfg, **exp_cfg}
    if 'rac_modules' in base_cfg and 'rac_modules' in exp_cfg: # Ensure nested dicts are merged
        config['rac_modules'] = {**base_cfg['rac_modules'], **exp_cfg['rac_modules']}
    print("Configuration loaded successfully.")
    # print(yaml.dump(config, indent=2))
except Exception as e:
    print(f"Error loading configuration: {e}")
    # sys.exit()

# Setup device and seed
DEVICE = get_device(config.get('device', 'cuda'))
set_seed(config.get('seed', 42))

print(f"Using device: {DEVICE}")

# # 3. Load CLIP Backbone and Preprocessor
# Load the chosen CLIP model. Its parameters should remain frozen.

try:
    clip_backbone, clip_preprocess_fn = load_clip_model(
        model_name=config['clip_model_name'],
        device=DEVICE,
        extract_multi_level=config['rac_modules']['mfa']['enabled'] and config['rac_modules']['mfa'].get('num_levels', 0) > 0,
        num_levels=config['rac_modules']['mfa'].get('num_levels', 4)
    )
    print(f"CLIP model '{config['clip_model_name']}' loaded successfully.")
except Exception as e:
    print(f"Error loading CLIP model: {e}")
    clip_backbone, clip_preprocess_fn = None, None
    # sys.exit()

# # 4. Prepare Dummy Data / Small Sample Data
# Create or load a very small batch of images and corresponding labels for testing.

# Option 1: Create dummy tensors
NUM_SAMPLES = 4 # Small batch size for testing
IMAGE_SIZE = config.get('image_size', 224)
NUM_CLASSES = config.get('num_classes', 2) # Example: authentic/counterfeit
CHANNELS = 3

# Dummy images (already preprocessed)
dummy_images = torch.randn(NUM_SAMPLES, CHANNELS, IMAGE_SIZE, IMAGE_SIZE).to(DEVICE)
dummy_labels = torch.randint(0, NUM_CLASSES, (NUM_SAMPLES,)).to(DEVICE)

# Dummy multi-level features (if MFA is used)
# This part is highly dependent on your CLIP backbone and how multi-level features are extracted.
# For ResNet50, feature_dim is 1024. Channels for MFA adapters would be intermediate layer channels.
# For ViT-B/16, feature_dim is 512.
dummy_multi_level_features = None
if config['rac_modules']['mfa']['enabled']:
    num_levels_mfa = config['rac_modules']['mfa'].get('num_levels', 4)
    # This is a placeholder. Actual shapes depend on the backbone and extraction points.
    # If adapters are applied to raw encoder outputs (before CLIP's projection):
    # img_enc_channels = config.get('image_encoder_channels', [256, 512, 1024, 2048]) # For RN50 example
    # dummy_multi_level_features = [
    #     torch.randn(NUM_SAMPLES, img_enc_channels[i], IMAGE_SIZE // (2**(i+2)), IMAGE_SIZE // (2**(i+2))).to(DEVICE)
    #     for i in range(num_levels_mfa)
    # ]
    # If adapters are applied to features already at CLIP's feature_dim (e.g., from ViT blocks):
    feature_dim = config['feature_dim']
    dummy_multi_level_features = [
        torch.randn(NUM_SAMPLES, feature_dim).to(DEVICE) # Assuming pooled features for adapters
        for _ in range(num_levels_mfa)
    ]
    # If your MFA takes (B, S, D) from ViT layers and then pools, adjust this.
    # For example, if MFA expects (B, NumTokens, Dim) from ViT:
    # num_tokens = (IMAGE_SIZE // 16)**2 + 1 # For ViT-B/16, patch_size=16
    # dummy_multi_level_features = [
    #    torch.randn(NUM_SAMPLES, num_tokens, feature_dim).to(DEVICE)
    #    for _ in range(num_levels_mfa)
    # ]


print(f"Prepared dummy images of shape: {dummy_images.shape}")
print(f"Prepared dummy labels of shape: {dummy_labels.shape}")
if dummy_multi_level_features:
    print(f"Prepared {len(dummy_multi_level_features)} dummy multi-level features.")
    for i, f in enumerate(dummy_multi_level_features):
        print(f"  Level {i} shape: {f.shape}")


# Option 2: Load a few actual images (Recommended for better testing)
# Create a list of a few image paths from your data/raw/
# REAL_IMAGE_PATHS = [
#     '../data/raw/authentic_product_A/image001.jpg',
#     '../data/raw/counterfeit_product_A/image001.jpg',
#     # Add a few more
# ]
# REAL_LABELS = [0, 1] # Corresponding labels
#
# processed_real_images = []
# if clip_preprocess_fn:
#     for img_path in REAL_IMAGE_PATHS:
#         try:
#             image = Image.open(img_path).convert("RGB")
#             processed_real_images.append(clip_preprocess_fn(image))
#         except FileNotFoundError:
#             print(f"Warning: Real image not found: {img_path}")
#
# if processed_real_images:
#     dummy_images = torch.stack(processed_real_images).to(DEVICE)
#     dummy_labels = torch.tensor(REAL_LABELS).to(DEVICE)
#     print(f"Loaded {len(processed_real_images)} real images for testing.")
# else:
#     print("Using dummy tensor data as real images were not loaded.")


# Generate Text Features for Prompts
# This part needs your actual class names and prompt generation logic.
# You'll need to create src.models.utils.generate_text_features_for_prompts
# For now, let's create dummy text features.
if 'class_names' in config and clip_backbone:
    try:
        # Create a dummy generate_text_features_for_prompts if not available for testing
        if not hasattr(sys.modules['src.models.utils'], 'generate_text_features_for_prompts'):
            def temp_generate_text_features(clip_model_wrapper, prompts, device_):
                print(f"DUMMY: Generating text features for {len(prompts)} prompts...")
                with torch.no_grad():
                    # text_tokens = clip.tokenize(prompts).to(device_) # Requires OpenAI's clip
                    # text_feat = clip_model_wrapper.model.encode_text(text_tokens)
                    text_feat = torch.randn(len(prompts), config['feature_dim']).to(device_) # Dummy
                    text_feat /= text_feat.norm(dim=-1, keepdim=True)
                return text_feat
            sys.modules['src.models.utils'].generate_text_features_for_prompts = temp_generate_text_features

        class_names = config.get('class_names', [f"class_{i}" for i in range(NUM_CLASSES)])
        text_prompts = [f"a photo of a {name.replace('_', ' ')}" for name in class_names]
        dummy_text_features_T = generate_text_features_for_prompts(clip_backbone, text_prompts, DEVICE)
        print(f"Generated dummy text features of shape: {dummy_text_features_T.shape}")
    except Exception as e_text_feat:
        print(f"Error generating text features: {e_text_feat}. Using random tensor.")
        dummy_text_features_T = torch.randn(NUM_CLASSES, config['feature_dim']).to(DEVICE)
        dummy_text_features_T = dummy_text_features_T / dummy_text_features_T.norm(dim=-1, keepdim=True)

else:
    print("Class names or CLIP backbone not available, creating random text features.")
    dummy_text_features_T = torch.randn(NUM_CLASSES, config['feature_dim']).to(DEVICE)
    dummy_text_features_T = dummy_text_features_T / dummy_text_features_T.norm(dim=-1, keepdim=True)
    print(f"Created random text features of shape: {dummy_text_features_T.shape}")


# # 5. Test Individual Model Components (Optional)
# This is useful for debugging sub-modules like MFA, IRA, ALF.

# ## 5.1 Test ZS-CLIP Module
print("\n--- Testing ZeroShotCLIPModule ---")
if clip_backbone:
    try:
        zs_clip_module = ZeroShotCLIPModule(clip_backbone.model) # Pass the underlying CLIP model
        zs_clip_module.to(DEVICE)
        zs_clip_module.eval()
        with torch.no_grad():
            s_ZS, z_i = zs_clip_module(dummy_images, dummy_text_features_T)
        print(f"s_ZS shape: {s_ZS.shape}, z_i shape: {z_i.shape}")
        assert s_ZS.shape == (NUM_SAMPLES, NUM_CLASSES)
        assert z_i.shape == (NUM_SAMPLES, config['feature_dim'])
    except Exception as e:
        print(f"Error testing ZS-CLIP Module: {e}")
else:
    print("CLIP backbone not loaded, skipping ZS-CLIP Module test.")

# ## 5.2 Test MFA Module
print("\n--- Testing MFAModule ---")
if config['rac_modules']['mfa']['enabled'] and dummy_multi_level_features:
    try:
        mfa = MFAModule(
            image_encoder_channels=config.get('image_encoder_channels'), # May be None if features are already at feature_dim
            feature_dim=config['feature_dim'],
            num_levels=config['rac_modules']['mfa'].get('num_levels', 4),
            fusion_type=config['rac_modules']['mfa'].get('fusion_type', 'WF'),
            projector_type=config['rac_modules']['mfa'].get('projector_type', 'identity'),
            adapter_bottleneck_ratio=config['rac_modules']['mfa'].get('adapter_bottleneck_ratio', 0.5),
            num_classes=NUM_CLASSES
        ).to(DEVICE)
        mfa.eval()
        with torch.no_grad():
            # The second argument to MFA in RACModel is global_clip_feature_z_i, which is z_i from ZS-CLIP
            # For standalone test, we can use a dummy one if z_i is not available from previous step.
            dummy_z_i_for_mfa = torch.randn(NUM_SAMPLES, config['feature_dim']).to(DEVICE) if 'z_i' not in locals() else z_i
            z_e_mfa, s_mfa = mfa(dummy_multi_level_features, dummy_z_i_for_mfa)
        print(f"z_e_mfa shape: {z_e_mfa.shape}, s_mfa shape: {s_mfa.shape}")
        assert z_e_mfa.shape == (NUM_SAMPLES, config['feature_dim'])
        assert s_mfa.shape == (NUM_SAMPLES, NUM_CLASSES)
    except Exception as e:
        print(f"Error testing MFA Module: {e}")
        # import traceback; traceback.print_exc() # For detailed error
else:
    print("MFA module disabled in config or dummy multi-level features not prepared. Skipping test.")

# ## 5.3 Test IRA Module
print("\n--- Testing IRAModule ---")
if config['rac_modules']['ira']['enabled']:
    try:
        # Inputs for IRA: s_ZS and z_e (from MFA)
        dummy_s_ZS_for_ira = torch.randn(NUM_SAMPLES, NUM_CLASSES).to(DEVICE) if 's_ZS' not in locals() else s_ZS
        dummy_z_e_for_ira = torch.randn(NUM_SAMPLES, config['feature_dim']).to(DEVICE) if 'z_e_mfa' not in locals() else z_e_mfa

        ira = IRAModule(
            num_classes=NUM_CLASSES,
            feature_dim=config['feature_dim'],
            adapter_bottleneck_ratio=config['rac_modules']['ira'].get('adapter_bottleneck_ratio', 0.5)
        ).to(DEVICE)
        ira.eval()
        with torch.no_grad():
            s_ira = ira(dummy_s_ZS_for_ira, dummy_z_e_for_ira)
        print(f"s_ira shape: {s_ira.shape}")
        assert s_ira.shape == (NUM_SAMPLES, NUM_CLASSES)
    except Exception as e:
        print(f"Error testing IRA Module: {e}")
else:
    print("IRA module disabled in config. Skipping test.")


# ## 5.4 Test ALF Module
print("\n--- Testing ALFModule ---")
if config['rac_modules']['alf']['enabled']:
    try:
        # Inputs for ALF: s_mfa, s_ira, z_e
        dummy_s_mfa_for_alf = torch.randn(NUM_SAMPLES, NUM_CLASSES).to(DEVICE) if 's_mfa' not in locals() else s_mfa
        dummy_s_ira_for_alf = torch.randn(NUM_SAMPLES, NUM_CLASSES).to(DEVICE) if 's_ira' not in locals() else s_ira
        dummy_z_e_for_alf = torch.randn(NUM_SAMPLES, config['feature_dim']).to(DEVICE) if 'z_e_mfa' not in locals() else z_e_mfa

        alf = ALFModule(
            feature_dim=config['feature_dim'],
            alpha_generator_bottleneck_ratio=config['rac_modules']['alf'].get('adapter_bottleneck_ratio', 0.5)
        ).to(DEVICE)
        alf.eval()
        with torch.no_grad():
            s_alf, alpha = alf(dummy_s_mfa_for_alf, dummy_s_ira_for_alf, dummy_z_e_for_alf)
        print(f"s_alf shape: {s_alf.shape}, alpha shape: {alpha.shape}")
        assert s_alf.shape == (NUM_SAMPLES, NUM_CLASSES)
        assert alpha.shape == (NUM_SAMPLES, 1)
    except Exception as e:
        print(f"Error testing ALF Module: {e}")
else:
    print("ALF module disabled in config. Skipping test.")


# # 6. Test Full RAC Model Forward Pass
# Instantiate the full RACModel and run a forward pass.

print("\n--- Testing Full RACModel ---")
if clip_backbone:
    try:
        rac_model_instance = RACModel(config, clip_backbone, num_classes=NUM_CLASSES).to(DEVICE)
        rac_model_instance.eval() # Set to evaluation mode for testing

        print(f"RACModel learnable parameters: {sum(p.numel() for p in rac_model_instance.get_learnable_parameters() if p.requires_grad)}")

        # Perform a forward pass
        with torch.no_grad():
            final_logits, total_loss, loss_dict = rac_model_instance(
                dummy_images,
                multi_level_features_f=dummy_multi_level_features, # Can be None if not used/needed
                text_features_T=dummy_text_features_T,
                labels_y=dummy_labels # Pass labels to check loss calculation
            )

        print(f"\nRACModel Forward Pass Output:")
        print(f"  Final Logits shape: {final_logits.shape}")
        if total_loss is not None:
            print(f"  Total Loss: {total_loss.item():.4f}")
            print(f"  Loss Dictionary: {loss_dict}")
        else:
            print("  Total Loss: Not computed (labels_y might have been None or an issue occurred).")

        assert final_logits.shape == (NUM_SAMPLES, NUM_CLASSES)
        if dummy_labels is not None:
            assert total_loss is not None, "Total loss should be computed when labels are provided."
            assert isinstance(loss_dict, dict)

        print("\nFull RACModel forward pass test completed successfully.")

    except Exception as e:
        print(f"Error testing Full RACModel: {e}")
        import traceback
        traceback.print_exc()
else:
    print("CLIP backbone not loaded, skipping Full RACModel test.")


# # 7. Test Model with Real Image (if loaded)
# if 'processed_real_images' in locals() and processed_real_images:
#     print("\n--- Testing Full RACModel with a Real Image Sample ---")
#     if clip_backbone:
#         try:
#             rac_model_instance.eval() # Ensure it's in eval mode
#             single_real_image_batch = dummy_images[0:1] # Taking the first real image used earlier
#             single_real_label_batch = dummy_labels[0:1]
#
#             # Extract multi-level features for the single real image if needed
#             single_real_multi_level_features = None
#             if config['rac_modules']['mfa']['enabled'] and clip_backbone.extract_multi_level:
#                 # This assumes your clip_backbone.encode_image can give multi-level features
#                 # And they are processed appropriately before passing to MFA
#                 with torch.no_grad():
#                     _, temp_ml_feats = clip_backbone.encode_image(single_real_image_batch)
#                     # This part needs careful alignment with how MFA expects its input features
#                     # The dummy_multi_level_features generation gives a hint.
#                     # For simplicity, we might skip this or use a placeholder if it's too complex for this script
#                     if temp_ml_feats and isinstance(temp_ml_feats, list):
#                         single_real_multi_level_features = [f[0:1] for f in temp_ml_feats if f.shape[0] >= 1] # take first sample from batch
#                         if not all(isinstance(f, torch.Tensor) for f in single_real_multi_level_features):
#                            single_real_multi_level_features = None # Fallback
#                     print(f"Prepared single real multi-level features (if any): "
#                           f"{[f.shape for f in single_real_multi_level_features] if single_real_multi_level_features else 'None'}")
#
#             with torch.no_grad():
#                 final_logits_real, _, _ = rac_model_instance(
#                     single_real_image_batch,
#                     multi_level_features_f=single_real_multi_level_features, # Adjust as per your feature extraction
#                     text_features_T=dummy_text_features_T,
#                     labels_y=None # No loss calculation for this quick test
#                 )
#             pred_label_real = torch.argmax(final_logits_real, dim=1).item()
#             true_label_real = single_real_label_batch.item()
#
#             print(f"Real Image Test:")
#             print(f"  Logits: {final_logits_real.squeeze().cpu().numpy()}")
#             print(f"  Predicted Label Index: {pred_label_real}, True Label Index: {true_label_real}")
#             if 'class_names' in config:
#                 print(f"  Predicted Class: {config['class_names'][pred_label_real]}, True Class: {config['class_names'][true_label_real]}")
#
#         except Exception as e:
#             print(f"Error testing RACModel with real image: {e}")
#             import traceback
#             traceback.print_exc()
#     else:
#         print("CLIP backbone not loaded, skipping real image test.")
# else:
#     print("\nNo real images loaded for testing, skipping real image sample test.")


# # 8. Conclusions from Testing
# - Did the forward pass run without errors?
# - Are the output shapes as expected?
# - Do loss values seem reasonable (e.g., not NaN or excessively large for initial random weights)?
# - This notebook helps catch bugs in model definitions, tensor shapes, and data flow.

print("\nModel testing notebook finished.")