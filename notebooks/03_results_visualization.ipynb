# In a Jupyter Notebook cell

# 03_results_visualization.ipynb

# # 1. Setup and Imports
# Import necessary libraries for data loading, analysis, and visualization.

import os
import sys
import yaml
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
import json # If you save metrics as JSON

# Add src to path to import custom modules (if needed for utility functions)
# Adjust the path as necessary if your notebook is not in the project root
module_path = os.path.abspath(os.path.join('..')) # Assuming notebooks are in a 'notebooks' folder
if module_path not in sys.path:
    sys.path.append(module_path)

# Custom utility imports (examples)
from src.utils.general_utils import load_yaml_config
from src.evaluation.metrics import plot_confusion_matrix, generate_confusion_matrix # From your project
from src.utils.visualization_utils import plot_training_history # From your project

# Set plot styles
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_palette("muted")

print("Imports completed.")

# # 2. Configuration and Paths to Results
# Define paths to where your experiment results, logs, and saved models are stored.

# Option 1: Manually define paths to specific experiment results
# RESULTS_ROOT_DIR = '../results/' # Main results directory
# LOGS_DIR = os.path.join(RESULTS_ROOT_DIR, 'logs')
# SAVED_MODELS_DIR = '../saved_models/'

# Example: Specify one or more experiment names to analyze
EXPERIMENT_NAMES = [
    "rac_resnet50_agricultural_product_A_16shot", # From your train.py example
    "rac_vit_b16_agricultural_product_B_8shot",  # From your train.py example
    # Add more experiment names as needed
]

# Base directories (adjust if your structure is different)
BASE_RESULTS_DIR = '../results/'
BASE_LOGS_DIR = os.path.join(BASE_RESULTS_DIR, 'logs')
BASE_SAVED_MODELS_DIR = '../saved_models/'


# Function to load training history if saved as CSV or from log files
def load_training_log_to_history(log_file_path):
    """
    Parses a log file to extract training and validation metrics per epoch.
    This is a simplified parser; you'll need to adapt it to your log file format.
    """
    history = {
        'train_loss_total': [], 'val_loss_total': [],
        'train_accuracy': [], 'val_accuracy': [],
        # Add other metrics you log, e.g., specific loss components
        'train_loss_ce_alf': [], 'val_loss_ce_alf': [],
        'train_loss_total_sim': [], 'val_loss_total_sim': []
    }
    epochs_processed_train = set()
    epochs_processed_val = set()

    try:
        with open(log_file_path, 'r') as f:
            for line in f:
                if "Epoch" in line and "Metrics" in line:
                    parts = line.split("Metrics:")[1].strip()
                    epoch_info = line.split("Epoch")[1].split(" ")[1].split("/")[0] # Get current epoch
                    epoch = int(epoch_info)

                    current_metrics = {}
                    for item in parts.split(","):
                        key_val = item.split(":")
                        if len(key_val) == 2:
                            key = key_val[0].strip()
                            try:
                                val = float(key_val[1].strip())
                                current_metrics[key] = val
                            except ValueError:
                                pass # Skip if value is not float

                    if "Train Metrics" in line:
                        if epoch not in epochs_processed_train: # Process each epoch once for train
                            history['train_loss_total'].append(current_metrics.get('train_loss_total'))
                            history['train_accuracy'].append(current_metrics.get('train_accuracy'))
                            history['train_loss_ce_alf'].append(current_metrics.get('train_loss_ce_alf'))
                            history['train_loss_total_sim'].append(current_metrics.get('train_loss_total_sim'))
                            epochs_processed_train.add(epoch)
                    elif "Validation Metrics" in line:
                         if epoch not in epochs_processed_val: # Process each epoch once for val
                            history['val_loss_total'].append(current_metrics.get('val_loss_total'))
                            history['val_accuracy'].append(current_metrics.get('val_accuracy'))
                            history['val_loss_ce_alf'].append(current_metrics.get('val_loss_ce_alf'))
                            history['val_loss_total_sim'].append(current_metrics.get('val_loss_total_sim'))
                            epochs_processed_val.add(epoch)
        # Filter out None values if some epochs didn't have all metrics
        for key in history:
            history[key] = [v for v in history[key] if v is not None]
        # Ensure all lists have the same length by padding shorter ones if necessary (or truncating longer ones)
        if history['train_loss_total'] and history['val_loss_total']: # only if both have data
            min_len = min(len(history['train_loss_total']), len(history['val_loss_total']))
            for key in history:
                history[key] = history[key][:min_len]

    except FileNotFoundError:
        print(f"Log file not found: {log_file_path}")
        return None
    except Exception as e:
        print(f"Error parsing log file {log_file_path}: {e}")
        return None
    return history

print("Setup for result loading and visualization complete.")

# # 3. Plot Training History (Loss & Accuracy Curves)
# Load and plot training history for each specified experiment.

for exp_name in EXPERIMENT_NAMES:
    print(f"\n--- Visualizing Training History for Experiment: {exp_name} ---")
    
    # Attempt to find the log file (assumes a naming convention like experiment_name_timestamp.log)
    log_file_path = None
    exp_log_dir = BASE_LOGS_DIR # Assuming logs are directly in BASE_LOGS_DIR named by experiment
    if os.path.isdir(exp_log_dir):
        # Find the most recent log file for this experiment name prefix
        matching_logs = sorted([f for f in os.listdir(exp_log_dir) if f.startswith(exp_name.replace(' ', '_')) and f.endswith(".log")], reverse=True)
        if matching_logs:
            log_file_path = os.path.join(exp_log_dir, matching_logs[0])
            print(f"Found log file: {log_file_path}")
        else:
            print(f"No log file found starting with '{exp_name}' in {exp_log_dir}")
            
    if log_file_path and os.path.exists(log_file_path):
        history = load_training_log_to_history(log_file_path)
        
        if history and history['train_loss_total']: # Check if history was successfully loaded and has data
            plot_keys_config = {
                'loss': ['train_loss_total', 'val_loss_total', 'train_loss_ce_alf', 'val_loss_ce_alf'],
                'accuracy': ['train_accuracy', 'val_accuracy'],
                'similarity_loss': ['train_loss_total_sim', 'val_loss_total_sim'] # Example for another axis or separate plot
            }
            
            history_plot_save_path = os.path.join(BASE_RESULTS_DIR, exp_name, "plots", "training_curves.png")
            plot_training_history(
                history,
                plot_keys={'loss': plot_keys_config['loss'], 'accuracy': plot_keys_config['accuracy']},
                title=f"Training History: {exp_name}",
                save_path=history_plot_save_path
            )
            # You might want a separate plot for other losses like similarity loss
            if history.get('train_loss_total_sim'):
                 plot_training_history(
                    history,
                    plot_keys={'loss': ['train_loss_total_sim', 'val_loss_total_sim']}, # Plotting sim loss on primary axis
                    title=f"Similarity Loss History: {exp_name}",
                    ylabel_primary="Similarity Loss",
                    save_path=os.path.join(BASE_RESULTS_DIR, exp_name, "plots", "similarity_loss_curves.png")
                )
        else:
            print(f"Could not plot training history for {exp_name} (no data or parse error).")
    else:
        print(f"Log file for {exp_name} not found or specified path is invalid.")

# # 4. Visualize Evaluation Results (e.g., Confusion Matrix)
# Load evaluation results (predictions, true labels) if saved, or re-run evaluation on test set.
# For this notebook, we'll assume you might have saved predictions or can re-run parts of evaluate.py.
# The `evaluate.py` script already saves confusion matrices. This section could be for:
# - Comparing CMs from different experiments.
# - Plotting other evaluation metrics (e.g., precision-recall curves).
# - Visualizing sample predictions if not done elsewhere.

for exp_name in EXPERIMENT_NAMES:
    print(f"\n--- Visualizing Evaluation for Experiment: {exp_name} ---")
    exp_dir = os.path.join(BASE_RESULTS_DIR, exp_name)
    plots_dir = os.path.join(exp_dir, "plots")
    
    # Check if confusion matrix already exists (was created by evaluate.py)
    cm_path = os.path.join(plots_dir, f"confusion_matrix_test_{exp_name}.png")
    if os.path.exists(cm_path):
        print(f"Displaying existing confusion matrix for {exp_name}:")
        try:
            img = Image.open(cm_path)
            plt.figure(figsize=(10,8)) # Adjust size as needed
            plt.imshow(img)
            plt.axis('off') # Hide axes for image display
            plt.title(f"Confusion Matrix: {exp_name} (from file)")
            plt.show()
        except Exception as e:
            print(f"Could not display image {cm_path}: {e}")
    else:
        print(f"Confusion matrix plot not found at {cm_path}. Run evaluate.py first or generate here.")
        # To generate here, you would need to:
        # 1. Load the best model for this experiment.
        # 2. Load the test dataset.
        # 3. Get predictions and true labels.
        # 4. Call generate_confusion_matrix and plot_confusion_matrix.
        # This involves re-running parts of evaluate.py logic.
        # For example:
        # model_path = os.path.join(BASE_SAVED_MODELS_DIR, exp_name, 'best_model.pth.tar')
        # config_path = os.path.join(BASE_SAVED_MODELS_DIR, exp_name, 'config_used.yaml') # If saved
        # if os.path.exists(model_path) and os.path.exists(config_path):
        #     print(f"Attempting to generate CM for {exp_name} from saved model...")
        #     # ... (Add logic similar to evaluate.py to load model, data, predict, and plot CM) ...
        # else:
        #     print(f"Cannot generate CM: model or config for {exp_name} not found.")


    # You could also load a CSV of predictions if your evaluate.py saves one
    # predictions_csv_path = os.path.join(exp_dir, "predictions", f"test_predictions_{exp_name}.csv")
    # if os.path.exists(predictions_csv_path):
    #     df_preds = pd.read_csv(predictions_csv_path)
    #     # Analyze df_preds, e.g., plot precision-recall curves per class
    #     # from sklearn.metrics import precision_recall_curve, average_precision_score
    #     # ... (plotting logic for PR curves) ...


# # 5. Compare Performance Across Experiments (Optional)
# If you have metrics saved from multiple experiments, you can create comparison plots.

# Example: Assume you have a summary CSV/JSON file with key metrics per experiment
# summary_metrics_path = os.path.join(BASE_RESULTS_DIR, "all_experiments_summary.csv")
# if os.path.exists(summary_metrics_path):
#     df_summary = pd.read_csv(summary_metrics_path) # Needs columns like 'experiment_name', 'test_accuracy', 'test_f1'
#     print("\n--- Comparing Experiment Performances ---")
    
#     plt.figure(figsize=(10, 6))
#     sns.barplot(x='experiment_name', y='test_accuracy', data=df_summary)
#     plt.title('Test Accuracy Comparison Across Experiments')
#     plt.xticks(rotation=45, ha="right")
#     plt.ylabel('Accuracy (%)')
#     plt.tight_layout()
#     comparison_plot_path = os.path.join(BASE_RESULTS_DIR, "experiment_accuracy_comparison.png")
#     plt.savefig(comparison_plot_path)
#     plt.show()
# else:
#     print("\nExperiment summary file not found. Skipping comparison plots.")


# # 6. Visualize Sample Misclassifications (Advanced)
# - Load a model and test data.
# - Identify misclassified samples.
# - Plot these samples with their true and predicted labels.
# This would use `plot_sample_images_with_predictions` from `visualization_utils.py`
# after filtering for misclassified samples from your evaluation run.

# # Example (conceptual, needs model loading and prediction logic from evaluate.py):
# for exp_name in EXPERIMENT_NAMES:
#     print(f"\n--- Visualizing Misclassifications for Experiment: {exp_name} ---")
#     # 1. Load best model for exp_name
#     # 2. Load test data
#     # 3. Get predictions (all_preds_np) and true labels (all_labels_np)
#     # 4. Get original images for test data (test_dataset.images or load from paths)
#     # 5. Find indices of misclassified samples:
#     #    misclassified_indices = np.where(all_preds_np != all_labels_np)[0]
#     #    if len(misclassified_indices) > 0:
#     #        sample_misclassified_indices = np.random.choice(misclassified_indices, size=min(8, len(misclassified_indices)), replace=False)
#     #        
#     #        misclassified_images = [test_dataset_images[i] for i in sample_misclassified_indices] # pseudocode
#     #        misclassified_true = all_labels_np[sample_misclassified_indices]
#     #        misclassified_pred = all_preds_np[sample_misclassified_indices]
#     #        class_names_from_config = # load from config
#     #
#     #        misclass_plot_path = os.path.join(BASE_RESULTS_DIR, exp_name, "plots", "misclassified_samples.png")
#     #        plot_sample_images_with_predictions(
#     #            misclassified_images, misclassified_true, misclassified_pred,
#     #            class_names_from_config, max_samples=8,
#     #            title=f"Misclassified Samples: {exp_name}",
#     #            save_path=misclass_plot_path
#     #        )
#     #    else:
#     #        print(f"No misclassifications found for {exp_name} to visualize (or data not loaded).")
#     pass


print("\nResults visualization notebook finished.")